{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "    index_name=\"engineering_fundamentals\",\n",
    "    embedding_function=embedding_model.embed_query,\n",
    ")\n",
    "\n",
    "message_history = ChatMessageHistory()\n",
    "conversation_buffer_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    output_key=\"answer\",\n",
    "    chat_memory=message_history,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chat_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    memory=conversation_buffer_memory,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a test set with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original data - split into chunks\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def get_md_files(directory):\n",
    "    files = []\n",
    "    for path in sorted(directory.rglob(\"*.md\")):\n",
    "        relative_path = path.relative_to(directory)\n",
    "        topics = str(relative_path).split(\"\\\\\")[:-1]\n",
    "        files.append([path, topics])\n",
    "    return files\n",
    "\n",
    "def get_chunks(path, topics):\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Title\"),\n",
    "        (\"##\", \"Subheader\")\n",
    "    ]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        try:\n",
    "            markdown_text = f.read()\n",
    "            chunks = markdown_splitter.split_text(markdown_text)\n",
    "        except Exception as ex:\n",
    "            print(path, ex)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"topics\"] = ','.join(topics)\n",
    "        chunk.metadata[\"path\"] = str(path)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# load the filenames and topics\n",
    "doc_dir = Path(\"../data/docs\")\n",
    "files = get_md_files(doc_dir)\n",
    "\n",
    "# load chunks from the files\n",
    "docs = []\n",
    "\n",
    "for path, topic in files:\n",
    "    docs += get_chunks(path, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "# ideally we should have a 4.0 evaluator for a 3.5 pipeline\n",
    "# but for the demo I'm using the same\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=chat_model,\n",
    "    critic_llm=chat_model,\n",
    "    embeddings=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    docs,\n",
    "    test_size=10,\n",
    "    raise_exceptions=False,\n",
    "    with_debugging_logs=False,\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = testset.to_pandas()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[0].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[0].ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[0].contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer the queries in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [dr.question for dr in testset.test_data]\n",
    "ground_truth = [dr.ground_truth for dr in testset.test_data]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in questions:\n",
    "    result = rag_chain.invoke(question)\n",
    "    answers.append(result[\"answer\"])\n",
    "    contexts.append([doc.page_content for doc in result[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truth,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "inference_dataset = Dataset.from_dict(inference_data)\n",
    "inference_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the inferred results\n",
    "\n",
    "> NOTE: We should ideally use a better eval model like 4.0 evaluating 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    llm=chat_model,\n",
    "    embeddings=embedding_model,\n",
    "    dataset = inference_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")\n",
    "\n",
    "evaluation_result = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the context recall is a bit on the low side - but we also have extremely few items in our test set so one bad apple throws it off -- here it is all the ones that have ground truth as nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
