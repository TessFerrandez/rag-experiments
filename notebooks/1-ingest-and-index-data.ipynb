{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files and extract topics for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_md_files(directory):\n",
    "    files = []\n",
    "    for path in sorted(directory.rglob(\"*.md\")):\n",
    "        relative_path = path.relative_to(directory)\n",
    "        topics = str(relative_path).split(\"\\\\\")[:-1]\n",
    "        files.append([path, topics])\n",
    "    return files\n",
    "\n",
    "\n",
    "doc_dir = Path(\"../data/docs\")\n",
    "files = get_md_files(doc_dir)\n",
    "\n",
    "for file, topic in files[:5]:\n",
    "    print(f\"{file} -> {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split files into chunks / langchain docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Title\"),\n",
    "    (\"##\", \"Subheader\")\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "def get_chunks(path, topics):\n",
    "    chunks = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        try:\n",
    "            markdown_text = f.read()\n",
    "            chunks = markdown_splitter.split_text(markdown_text)\n",
    "        except Exception as ex:\n",
    "            print(path, ex)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata[\"topics\"] = ','.join(topics)\n",
    "        chunk.metadata[\"path\"] = str(path)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "docs = []\n",
    "\n",
    "for path, topic in files:\n",
    "    docs += get_chunks(path, topic)\n",
    "\n",
    "for doc in docs[:5]:\n",
    "    print(doc)\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed the docs and add them to a vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embedding model\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    api_version=\"2023-05-15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample embedding for the text \"hello world\"\n",
    "test_text = \"Hello, world!\"\n",
    "embedding = embedding_model.embed_query(test_text)\n",
    "\n",
    "print(f\"# features: {len(embedding)}\")\n",
    "print(f\"first 5 features: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the vector store and the index (I picked the index name ef_docs for engineering fundamentals docs)\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "index_name = \"ef_docs\"\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    azure_search_key=os.getenv(\"AZURE_SEARCH_KEY\"),\n",
    "    index_name=index_name,\n",
    "    embedding_function=embedding_model.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the docs - this will embed them as well\n",
    "vector_store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the agile ceremonies\"\n",
    "result_docs = vector_store.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "for doc in result_docs:\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"TITLE:\", doc.metadata[\"Title\"])\n",
    "    print(\"TOPICS:\", doc.metadata[\"topics\"])\n",
    "    print(\"PATH:\", doc.metadata[\"path\"])\n",
    "    display(Markdown(doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
